version: '3.4'

x-spark-default-env:
  &spark-default-env
  SPARK_NO_DAEMONIZE: true
  SPARK_SCALA_VERSION: 2.13

x-spark-default:
  &spark-default
  env_file: .env
  volumes:
    - ./conf/app.properties:/opt/spark/conf/app.properties
    # - ./conf/log4j2.properties:/opt/spark/conf/log4j2.properties # optional
    - ./conf/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf

volumes:
  mosquitto_data:
  zookeeper_data:
  zookeeper_log:
  kafka_data:
  opensearch-node1:
  opensearch-node2:

services:
  mosquitto:
    image: eclipse-mosquitto:2.0.15
    volumes:
      - ./conf/mosquitto.conf:/mosquitto/config/mosquitto.conf:ro
      - ./conf/mosquitto_passwd:/mosquitto/config/password_file:ro
      - mosquitto_data:/mosquitto/data
    ports:
      - 1883:1883
    deploy:
      mode: replicated
      replicas: 1
      restart_policy:
        condition: on-failure
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 128M

  mqtt-source:
    image: mataelang/kafka-mqtt-source:1.1
    container_name: mqtt-source
    environment:
      MQTT_HOST: mosquitto
      MQTT_PORT: 1883
      MQTT_USERNAME:
      MQTT_PASSWORD:
      MQTT_TOPIC: mataelang/sensor/v3/+
      KAFKA_BOOSTRAP_SERVERS: kafka:9092
      KAFKA_PRODUCE_TOPIC: sensor_events
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 64M
        reservations:
          cpus: '0.25'
          memory: 32M

  zookeeper:
    image: confluentinc/cp-zookeeper:7.3.0
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    volumes:
      - zookeeper_data:/var/lib/zookeeper/data
      - zookeeper_log:/var/lib/zookeeper/log
    deploy:
      mode: replicated
      replicas: 1
      restart_policy:
        condition: on-failure
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M

  kafka:
    image: confluentinc/cp-kafka:7.3.0
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
    volumes:
      - kafka_data:/var/lib/kafka/data
    deploy:
      mode: replicated
      replicas: 1
      resources:
        limits:
          cpus: '0.5'
          memory: 2G
        reservations:
          cpus: '0.25'
          memory: 1G

  control-center:
    image: provectuslabs/kafka-ui
    container_name: control-center
    depends_on:
      - zookeeper
      - kafka
      - mqtt-source
    ports:
      - "9021:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: MataElangKafkaCluster
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:9092
      KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181
    deploy:
      resources:
        limits:
          cpus: '0.50'
          memory: 768M
        reservations:
          cpus: '0.25'
          memory: 384M

  spark-master:
    <<: *spark-default
    image: mataelang/spark:3.3.1-scala2.13
    depends_on:
      - kafka
    ports:
      - "8180:8080"
    environment:
      <<: *spark-default-env
      SPARK_MASTER_HOST: spark-master
      SPARK_MASTER_PORT: 7077
      SPARK_MASTER_WEBUI_PORT: 8080
      SPARK_DAEMON_MEMORY: 1g
    command: >
      bash -c "/opt/spark/sbin/start-master.sh"
    deploy:
      mode: replicated
      replicas: 1
      resources:
        limits:
          cpus: '1'
          memory: 1G
        reservations:
          cpus: '1'
          memory: 1G

  spark-worker:
    <<: *spark-default
    image: mataelang/spark:3.3.1-scala2.13
    depends_on:
      - spark-master
    # ports:
    #   - "8181-8182:8081"
    #   - "4041-4042:4040"
    environment:
      <<: *spark-default-env
      SPARK_MASTER: spark://spark-master:7077
      SPARK_WORKER_CORES: 1
      SPARK_WORKER_MEMORY: 1G
      SPARK_WORKER_DIR: /opt/spark/work-dir
    command: >
      bash -c "/opt/spark/sbin/start-worker.sh -h $(hostname -I | cut -d' ' -f1) $$SPARK_MASTER"
    deploy:
      mode: replicated
      replicas: 2

  spark-worker-bigmemory:
    <<: *spark-default
    image: mataelang/spark:3.3.1-scala2.13
    depends_on:
      - spark-master
      - spark-worker
    # ports:
    #   - "8183-8184:8081"
    #   - "4043-4044:4040"
    environment:
      <<: *spark-default-env
      SPARK_MASTER: spark://spark-master:7077
      SPARK_WORKER_CORES: 1
      SPARK_WORKER_MEMORY: 2G
      SPARK_WORKER_DIR: /opt/spark/work-dir
    command: >
      bash -c "sleep 5 && /opt/spark/sbin/start-worker.sh $$SPARK_MASTER"
    deploy:
      mode: replicated
      replicas: 2

  spark-historyserver:
    <<: *spark-default
    image: mataelang/spark:3.3.1-scala2.13
    depends_on:
      - spark-master
    ports:
      - target: 18080
        published: 18080
        protocol: tcp
        mode: host
    environment:
      <<: *spark-default-env
      SPARK_DAEMON_MEMORY: 1G
    command: >
      bash -c "/opt/spark/sbin/start-history-server.sh"
    deploy:
      mode: replicated
      replicas: 1
      resources:
        limits:
          cpus: '1'
          memory: 1G
        reservations:
          cpus: '1'
          memory: 1G

  spark-submit-enrich:
    <<: *spark-default
    image: mataelang/spark:3.3.1-scala2.13
    restart: "no"
    depends_on:
      - spark-master
      - spark-worker
    working_dir: /opt/spark
    environment:
      <<: *spark-default-env
      DEPLOY_MODE: cluster
      TOTAL_EXECUTOR_CORES: 1
      SPARK_DRIVER_MEMORY: 1g
      SPARK_EXECUTOR_CORES: 1
      SPARK_EXECUTOR_MEMORY: 1g
      SPARK_APP_CLASSNAME: org.mataelang.kaspacore.jobs.SensorEnrichDataStreamJob
    command: >
      bash -c "sleep 10 && /opt/spark/bin/spark-submit \\
          --total-executor-cores $$TOTAL_EXECUTOR_CORES \\
          --conf spark.submit.deployMode=$$DEPLOY_MODE \\
          --conf spark.driver.memory=$$SPARK_DRIVER_MEMORY \\
          --conf spark.executor.cores=$$SPARK_EXECUTOR_CORES \\
          --conf spark.executor.memory=$$SPARK_EXECUTOR_MEMORY \\
          --conf spark.eventLog.dir=$$SPARK_EVENTLOG_DIR \\
          --files conf/app.properties \\
          $$SPARK_APP_JAR_PATH
          "

  spark-submit-aggr:
    <<: *spark-default
    image: mataelang/spark:3.3.1-scala2.13
    restart: "no"
    depends_on:
      - spark-master
      - spark-worker-bigmemory
    working_dir: /opt/spark
    environment:
      <<: *spark-default-env
      DEPLOY_MODE: cluster
      TOTAL_EXECUTOR_CORES: 1
      SPARK_DRIVER_MEMORY: 2g
      SPARK_EXECUTOR_CORES: 1
      SPARK_EXECUTOR_MEMORY: 2g
      SPARK_SHUFFLE_PARTITION: 1
      SPARK_APP_CLASSNAME: org.mataelang.kaspacore.jobs.SensorAggregationStreamJob
    command: >
      bash -c "sleep 10 && /opt/spark/bin/spark-submit \\
          --class $$SPARK_APP_CLASSNAME \\
          --total-executor-cores $$TOTAL_EXECUTOR_CORES \\
          --conf spark.submit.deployMode=$$DEPLOY_MODE \\
          --conf spark.driver.memory=$$SPARK_DRIVER_MEMORY \\
          --conf spark.executor.cores=$$SPARK_EXECUTOR_CORES \\
          --conf spark.executor.memory=$$SPARK_EXECUTOR_MEMORY \\
          --conf spark.eventLog.dir=$$SPARK_EVENTLOG_DIR \\
          --conf spark.sql.shuffle.partitions=$$SPARK_SHUFFLE_PARTITION \\
          --conf spark.sql.codegen.aggregate.map.twolevel.enabled=false \\
          --conf spark.sql.streaming.metricsEnabled=true \\
          --files conf/app.properties \\
          $$SPARK_APP_JAR_PATH
          "

  opensearch-node1:
    image: opensearchproject/opensearch:2.4.0
    environment:
      - cluster.name=opensearch-cluster
      - node.name=opensearch-node1
      - discovery.seed_hosts=opensearch-node1,opensearch-node2
      - cluster.initial_cluster_manager_nodes=opensearch-node1,opensearch-node2
      - bootstrap.memory_lock=true
      - "OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m"
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    volumes:
      - opensearch-data1:/usr/share/opensearch/data
    ports:
      - 9200:9200
      
  opensearch-node2:
    image: opensearchproject/opensearch:2.4.0
    environment:
      - cluster.name=opensearch-cluster
      - node.name=opensearch-node2
      - discovery.seed_hosts=opensearch-node1,opensearch-node2
      - cluster.initial_cluster_manager_nodes=opensearch-node1,opensearch-node2
      - bootstrap.memory_lock=true
      - "OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m"
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    volumes:
      - opensearch-data2:/usr/share/opensearch/data

  opensearch-dashboards:
    image: opensearchproject/opensearch-dashboards:2.4.0
    ports:
      - 5601:5601
    environment:
      OPENSEARCH_HOSTS: '["https://opensearch-node1:9200","https://opensearch-node2:9200"]'

  opensearch-logstash:
    image: opensearchproject/logstash-oss-with-opensearch-output-plugin:8.4.0
    command: "-f /usr/share/logstash/config/pipeline.conf"
    ports:
      - "5044:5044"
      - "5000:5000/tcp"
      - "5000:5000/udp"
      - "9600:9600"
    environment:
      LS_JAVA_OPTS: -Xmx256m -Xms256m
      LOGSTASH_INTERNAL_PASSWORD: ${LOGSTASH_INTERNAL_PASSWORD:-}
    volumes:
      - ./conf/pipeline.conf:/usr/share/logstash/config/pipeline.conf
